---
title: "ğŸ•’ ElastiCache Redis Service Update: Real-World Timeline & What Each Event Means"
date: 2025-09-15
description: "A minute-by-minute breakdown of an ElastiCache for Redis service update (cluster mode disabled) with primary + replica. Includes the exact sequence, expected downtime window, and what to monitor."
categories:
  - aws
  - redis
  - elasticache
  - maintenance
  - high-availability
tags:
  - aws
  - elasticache
  - redis
  - maintenance
  - failover
  - sres
  - devops
---

This post documents a **live service update** on an ElastiCache for Redis replication group with **cluster mode disabled** (single shard, one primary + one replica).

All timestamps below are **Europe/London (UTC+01:00)**.

> **TL;DR:** The full maintenance window ran **23m16s** end-to-end, with a **brief failover blip** at 21:16:15 (typically seconds). Everything else was non-disruptive.

---

## ğŸ§­ Topology & Update Pattern

- **Topology:** 1 shard â†’ **Primary** (`node-primary`) + **Replica** (`node-replica`)
- **Update method (rolling):** 
  - 1) Patch replica  
  - 2) **Failover** (replica promoted to primary) â† *brief interruption here*  
  - 3) Patch the old primary (now replica)  
- **Blast radius:** With a healthy replica and automatic failover, the only noticeable impact is a short reconnect window during failover.

---

## â±ï¸ Timeline at a Glance

```
21:03:37   Replica begins patch ("Recovering")
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 9m53s â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
21:13:30   Replica patched ("Finished recovery") â€“ ready to be promoted
      â””â”€â”€ 2m45s â”€â”€â”˜
21:16:15   FAILOVER COMPLETED  â† brief client interruption here (seconds)
            â””â”€ 39s â”€â”˜
21:16:54   Old primary (now replica) begins patch ("Recovering")
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 9m59s â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
21:26:53   Old primary patched ("Finished recovery") â€“ replication healthy
```

**Durations (precise):**
- Replica patch: **9m53s** (21:03:37 â†’ 21:13:30)  
- Prep â†’ Failover: **2m45s** (21:13:30 â†’ 21:16:15)  
- Failover â†’ next patch start: **39s** (21:16:15 â†’ 21:16:54)  
- Old primary patch: **9m59s** (21:16:54 â†’ 21:26:53)  
- **Total window:** **23m16s** (21:03:37 â†’ 21:26:53)

---

## ğŸ“œ Event-by-Event: Whatâ€™s Actually Happening

> Iâ€™ve generalized the event text and node names to keep this reusable.

1) **21:03:37 â€” `node-replica`: â€œRecovering cache nodes 0001â€**  
   - AWS takes the **replica** out of rotation to apply the service update.  
   - **Impact:** None to traffic. Primary continues serving reads/writes.

2) **21:13:30 â€” `node-replica`: â€œFinished recovery for cache nodes 0001â€**  
   - Replica is patched, healthy, and in sync. Itâ€™s ready to be promoted.  
   - **Impact:** None (still running on the original primary).

3) **21:16:15 â€” Replication group + `node-primary`: â€œFailover â€¦ completedâ€**  
   - AWS promotes the updated replica to **new primary**.  
   - **This is the only interruption window**: clients briefly reconnect to the new primary endpoint.  
   - **Observed/expected:** seconds; typically visible as a short spike in connection errors/latency.

4) **21:16:54 â€” `node-primary` (now the replica): â€œRecovering cache nodes 0001â€**  
   - The old primary, now acting as the replica, is patched.  
   - **Impact:** None (the new primary keeps serving traffic).

5) **21:26:53 â€” `node-primary` (replica): â€œFinished recovery for cache nodes 0001â€**  
   - The second node is patched and replication is healthy again.  
   - **Maintenance complete.**

> You may see **duplicate â€œFailover completedâ€** entries at both the **replication group** and **cache-cluster** levels. Thatâ€™s normal.

---

## ğŸ§ª What to Monitor (and What â€œGoodâ€ Looks Like)

- **During failover (â‰ˆ seconds):**  
  - *Clients:* brief `ECONNRESET`/`READONLY`/timeout blips; auto-retries should succeed.  
  - *Endpoints:* If you use the **primary endpoint**, your client should reconnect automatically.

- **CloudWatch metrics (per node and/or replication group):**  
  - `CurrConnections` â€” small dip/spike at failover, then normalizes.  
  - `ReplicationLag` â€” near zero after each node rejoins (may momentarily increase as the second node catches up post-patch).  
  - `EngineCPUUtilization` / `CPUUtilization` â€” brief transient changes are fine; look for quick stabilization.  
  - `NetworkBytesIn/Out` â€” returns to baseline post-failover.

- **Quick Redis checks (from an app bastion):**
  - `INFO REPLICATION` â€” confirm `role:master` on the new primary; the peer shows as `slave`/`replica` and `state=online`.  
  - `ROLE` â€” fast sanity check for master/replica roles.  
  - A simple **SET/GET** smoke test against the primary endpoint.

---

## ğŸ§¯ Downtime, Risk, and Expectations

- With **cluster mode disabled** but a **healthy replica** and **automatic failover**:
  - **User-visible impact:** a short reconnect window at the failover timestamp (here: **21:16:15**).  
  - **Everything else:** non-disruptive patching while the other node carries traffic.
- If you had **no replica**, the node would be restarted in place â†’ **full downtime** during patch.

---

## âœ… Post-Maintenance Health Checklist (copy/paste)

1. **Endpoints & roles**
   - Connect to the **primary endpoint**, run `ROLE` or `INFO REPLICATION` â†’ verify `master` role.
2. **Replication**
   - On the replica: `INFO REPLICATION` â†’ `master_link_status:up`, `master_sync_in_progress:0`, `slave_repl_offset` close to master.
   - CloudWatch `ReplicationLag` trending to near-zero.
3. **Traffic**
   - App error rates/latency back to baseline within minutes of failover.
   - No sustained `READONLY` or timeout errors in logs.
4. **Capacity**
   - `CPUUtilization`, `FreeableMemory`, `Evictions`, `BytesUsedForCache` within normal bounds.
5. **Events**
   - Last events show both nodes â€œFinished recovery â€¦â€ with no subsequent warnings.

---

## ğŸ§  Takeaways & Benchmarks

- **Total maintenance window:** **~23 minutes** is a reasonable end-to-end expectation for a 2-node shard.  
- **User-visible impact:** **seconds**, concentrated at the **failover** moment.  
- **Consistency:** Both node patch phases were ~10 minutes each, with <3 minutes prep between first recovery and failover.

If your window is dramatically longer or you see lingering replication lag, dig into:
- Suboptimal client retry/backoff settings  
- Network path issues (look for elevated packet loss)  
- A replica catching up a large write burst right after failover

---

## ğŸ“ Reuse This Pattern

You can treat this as a template any time you apply **service updates** or do **planned failovers** on ElastiCache for Redis with **cluster mode disabled**:
- Patch replica â†’ **Failover** â†’ Patch old primary â†’ Verify health.

For **cluster mode enabled**, the pattern is the same but **per-shard**. Only the shard being updated experiences the brief blip; others continue serving normally.

---
